# 池化层

- 对输入图像下采样（压缩）
- 使特征图变小，简化网络计算复杂度
- 特征压缩，提取主要特征
- 相当于进行了非线性操作

最大池化、平均池化

# 上采样层

图像缩放：最临近插值、双线性插值

```python
torch.nn.functional.interpolate(
    input, 
    size=None, 
    scale_factor=None, 
    mode='nearest', 
    align_corners=None, 
    recompute_scale_factor=None
)
```



放大：转置卷积

```python
torch.nn.ConvTranspose2d(
    in_channels: int, 
    out_channels: int, 
    kernel_size: Union[T, Tuple[T, T]], 
    stride: Union[T, Tuple[T, T]] = 1, 
    padding: Union[T, Tuple[T, T]] = 0, 
    output_padding: Union[T, Tuple[T, T]] = 0, 
    groups: int = 1, 
    bias: bool = True, 
    dilation: int = 1, 
    padding_mode: str = 'zeros'
)
```



# 激活层

- 增加网络的非线性表达能力



# BN层

- 通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布
- 归—化的一种手段，它会减小图像之间的绝对差异，突出相对差异，加快训练速度

不适用：对噪声敏感的任务



# FC层

- 最为传统的神经网络，每个输入和每个输出都连接
- 可以认为是 embedding 的过程，相当于做了个映射
- 可以使用 1 x 1 的卷积 + 全局平局池化 来代替

```python
torch.nn.Linear(
    in_features: int, 
    out_features: int, 
    bias: bool = True
)
```



# Dropout层

训练时随机失活一部分神经元，测试的时候不使用随机失活

一般用在全连接层（传统的神经网络）



# 损失层

定义的损失函数

分类：

- 交叉熵，BCELoss（二分类的交叉熵）等

回归问题

- L1Loss、MSELoss等